# We can split large files into small shards to avoid memory overflow and boost process speed.
# specify number of shards to be processed in map function. Depends on your memory.
# If you don't want to split, you can set num_shards to 1.
# the larger dataset you have, the larger num_shards you need.
# Note that it must be multiples of num_worker. e.g., 1*num_workers, 2*num_workers
num_shards: 8
# if you have multiple (e.g., 4) GPUs, you can specify some of them(e.g. 0, 1, 2) or all of them (e.g., all)
cuda_devices: all  # specify visible GPU device. If you have multiple GPUs, you can specify some of them (e.g. 0, 1, 2) or all of them (e.g., all)
data_dir: ./data  # data directory

type_cfg:  # UFER type config
  work_dir: ./data/ufer_types  # work directory for UFER type
  original_type_vocab: original_type_vocab.txt  # original type vocabulary file, which stores original type words in UFER
  type_vocab: type_vocab.txt  # type vocabulary file, which stores all the output type words in UFER
  type_info_file: type_info.jsonl  # type information file, which stores information of all the types in UFER
  cache_info_file_0: 0_cache_none_type_info.jsonl  # the cache file for the first cache_stage
  cache_info_file_1: 1_cache_no_none_type_info.jsonl  # the cache file for the second cache_stage
  cache_info_file_2: 2_cache_disambiguation_type_info.jsonl  # the cache file for the third cache_stage

  # 1. dictionary api, which is used to get professional information for type words
  # we use a free dictionary api {https://github.com/meetDeveloper/freeDictionaryAPI}
  api_url: https://api.dictionaryapi.dev/api/v2/entries/  # API URL of the free Dictionary API
  lang_type: en  # language type of the dictionary you want, 'en' for English dictionary

  # 2. wikipedia api config, which is used to get wikipedia information for type words
  # see https://wikipedia.readthedocs.io/en/latest/quickstart.html for more details
  sentences: 2  # number of sentences to be extracted from Wikipedia Encyclopedia
  auto_suggest: False  # whether to automatically search for alternative suggestions if the query word is not found. Default is False.

  # 3. clustering config
  emb_model: ./ckpt/princeton-nlp/sup-simcse-roberta-large # embedding model, see detail at https://github.com/princeton-nlp/SimCSE
  emb_bs: 400  # batch size for embedding model
  local_files_only: True  # If True, offline mode. https://huggingface.co/docs/transformers/installation#offline-mode

  # Bisecting K-Means config
  # see detail at https://scikit-learn.org/stable/modules/generated/sklearn.cluster.BisectingKMeans.html#sklearn.cluster.BisectingKMeans
  n_clusters: 400  # cluster number for clustering. The smaller the value, the more accurate the disambiguation result will be。
  init: k-means++ # Method for initialization, we can choose ('k-means++', 'random', 'callable') or
  n_init: 3  # Number of time the inner k-means algorithm will be run with different centroid seeds in each bisection.
  random_state: 16  # the random seed, Use an int to make the randomness deterministic
  bisecting_strategy: largest_cluster  # Defines how bisection should be performed; choose from {“biggest_inertia”, “largest_cluster”}
  cluster_res_cache_file: cluster_results.csv  # cache cluster results file

  # 4. judge (by LLMs) config
  jud_models:
    - Mistral
    - Qwen
    #- Yi
    # - add more
    # you can add more judgment models in the future in a form of the judge model configs like below
    # all keys in the judge model configs are required!

  # judge model configs
  Mistral:
    # https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2
    checkpoint: ./ckpt/Mistral-7B-Instruct-v0.2  # your path to the model checkpoint
    # https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2#instruction-format
    jud_usr_prompt: >-
      You are a professional and helpful English linguist. 
      You diligently complete tasks as instructed. 
      You are about to read two definitions (i.e., definition A and definition B), which are followed by examples and 
      delimited by triple dashes (i.e., ---). 
      Please judge whether two definitions have similar meanings or explaining a same noun word by performing the following actions: 
      1) Think the first question carefully: Are the meanings of these two definitions similar? 
      2) Think the second question carefully: Are the two definitions explaining a same noun?  
      3) If you answer "Yes" to one of the above questions, only output 1; Otherwise, only output 0.
      4) Please note that expect for 0 or 1, all other text, punctuation, numbers, and whitespace characters are not allowed in the output.
  
      ### Start examples
      definition A: Any great, strong, powerful emotion, especially romantic love or extreme hate.
      definition B: A passionate individual.
      0
  
      definition A: A cargo ship or freighter is a merchant ship that carries cargo, goods, and materials from one port to another. Thousands of cargo carriers ply the world's seas and oceans each year, handling the bulk of international trade.
      definition B: Any vessel designed to carry cargo.
      1
    
      definition A: A flammable liquid consisting of a mixture of refined petroleum hydrocarbons, mainly used as a motor fuel; petrol.
      definition B: Petroleum, a fluid consisting of a mixture of refined petroleum hydrocarbons, primarily consisting of octane, commonly used as a motor fuel.
      1
    
      definition A: A shop or store that sells groceries; a grocery store.
      definition B: a shop that sells cakes.
      0
      ### End examples
      
      ---
      definition A: {first_definition} 
      definition B: {second_definition}
      ---

    jud_verify_prompt: >-
      Your answer is not allowed. Please think again and only output 0 or 1.

    jud_temperature: 0.1  # temperature for this model. We expect the judge model output deterministic results, so we set temperature to 0.1.
    jud_top_p: 0.6  # top_p for this model. The smaller the value, the more deterministic the model output is.
    jud_max_tokens: 3  # We only need to output 1 or 0, so we set max_tokens to 5.
    jud_bs: 24  # batch size for this model
    tensor_parallel_size: 2  # the number of GPUs you want to use for running this model using multi-GPU inference.
    dtype: auto  # https://docs.vllm.ai/en/latest/models/engine_args.html#cmdoption-dtype
    jud_res_cache_file: jud_results.csv  # cache judgment results file
    ver_res_cache_file: verify_results.csv  # the file for caching the judge results after verification. Each judge model will have a such file.
    verify_convs_cache_file: verify_convs.csv  # the file for caching the verification conversations. Each judge model will have a such file.

  #  ==========================================
  Qwen:
    # https://huggingface.co/Qwen/Qwen1.5-14B-Chat
    checkpoint: ./ckpt/Qwen/Qwen1.5-14B-Chat  # your path to the model checkpoint
    # qwen has system prompt. We can input the examples in a form of chatting
    # https://huggingface.co/Qwen/Qwen1.5-14B-Chat#quickstart
    jud_sys_prompt: >-
      You are a professional and helpful English linguist. 
      You are about to read two definitions (i.e., definition A and definition B).
      Please judge whether two definitions have similar meanings or explaining a same noun word by performing the following actions: 
      1) Think the first question carefully: Are the meanings of these two definitions similar? 
      2) Think the second question carefully: Are the two definitions explaining a same noun?  
      3) If you answer "Yes" to one of the above questions, only output 1; Otherwise, only output 0.
      4) Please note that expect for 0 or 1, all other text, punctuation, numbers, and whitespace characters are not allowed in the output.

    # the examples can be input in a form of chatting
    jud_examples:  # examples input in a form of chatting
      -
        definition_A: Any great, strong, powerful emotion, especially romantic love or extreme hate.
        definition_B: A passionate individual.
        output: 0
      -
        definition_A: A cargo ship or freighter is a merchant ship that carries cargo, goods, and materials from one port to another. Thousands of cargo carriers ply the world's seas and oceans each year, handling the bulk of international trade.
        definition_B: Any vessel designed to carry cargo.
        output: 1
      -
        definition_A: A flammable liquid consisting of a mixture of refined petroleum hydrocarbons, mainly used as a motor fuel; petrol.
        definition_B: Petroleum, a fluid consisting of a mixture of refined petroleum hydrocarbons, primarily consisting of octane, commonly used as a motor fuel.
        output: 1
      -
        definition_A: A shop or store that sells groceries; a grocery store.
        definition_B: a shop that sells cakes.
        output: 0
    jud_usr_prompt: >-
      definition A: {first_definition} 
      definition B: {second_definition}

    jud_verify_prompt: >-
      Your answer is not allowed. Please think again and only output 0 or 1. 

    jud_temperature: 0.1  # temperature for this model. We expect the judge model output deterministic results, so we set temperature to 0.1.
    jud_top_p: 0.6  # top_p for this model. The smaller the value, the more deterministic the model output is.
    jud_max_tokens: 3  # We only need to output 1 or 0, so we set max_tokens to 5.
    jud_bs: 24  # batch size for this model
    tensor_parallel_size: 2  # the number of GPUs you want to use for running this model using multi-GPU inference.
    dtype: auto  # https://docs.vllm.ai/en/latest/models/engine_args.html#cmdoption-dtype
    jud_res_cache_file: jud_results.csv  # cache judgment results file
    ver_res_cache_file: verify_results.csv  # the file for caching the judge results after verification. Each judge model will have a such file.
    verify_convs_cache_file: verify_convs.csv  # the file for caching the verification conversations. Each judge model will have a such file.

  # ==========================================
  Yi:  # The evaluation effect of Yi-6B-Chat is very poor
    checkpoint: ./ckpt/01-ai/Yi-6B-Chat  # https://huggingface.co/01-ai/Yi-6B-Chat
    # https://huggingface.co/01-ai/Yi-6B-Chat#perform-inference-with-yi-chat-model
    jud_usr_prompt: >-
      You are a professional and helpful English linguist. 
      You diligently complete tasks as instructed. 
      You are about to read two definitions (i.e., definition A and definition B), which are followed by examples and 
      delimited by triple dashes (i.e., ---). 
      Please judge whether two definitions have similar meanings or explaining a same noun word by performing the following actions: 
      1) Think the first question carefully: Are the meanings of these two definitions similar? 
      2) Think the second question carefully: Are the two definitions explaining a same noun?  
      3) If you answer "Yes" to one of the above questions, only output 1; Otherwise, only output 0.
      4) Please note that expect for 0 or 1, all other text, punctuation, numbers, and whitespace characters are not allowed in the output.

      ### Start examples
      definition A: Any great, strong, powerful emotion, especially romantic love or extreme hate.
      definition B: A passionate individual.
      0

      definition A: A cargo ship or freighter is a merchant ship that carries cargo, goods, and materials from one port to another. Thousands of cargo carriers ply the world's seas and oceans each year, handling the bulk of international trade.
      definition B: Any vessel designed to carry cargo.
      1

      definition A: A flammable liquid consisting of a mixture of refined petroleum hydrocarbons, mainly used as a motor fuel; petrol.
      definition B: Petroleum, a fluid consisting of a mixture of refined petroleum hydrocarbons, primarily consisting of octane, commonly used as a motor fuel.
      1

      definition A: A shop or store that sells groceries; a grocery store.
      definition B: a shop that sells cakes.
      0
      ### End examples

      ---
      definition A: {first_definition} 
      definition B: {second_definition}
      ---

    jud_verify_prompt: >-
      Your answer is not allowed. Please think again and only output 0 or 1. 

    jud_temperature: 0.1  # temperature for this model. We expect the judge model output deterministic results, so we set temperature to 0.1.
    jud_top_p: 0.6  # top_p for this model. The smaller the value, the more deterministic the model output is.
    jud_max_tokens: 3  # We only need to output 1 or 0, so we set max_tokens to 5.
    jud_bs: 24  # batch size for this model
    tensor_parallel_size: 2  # the number of GPUs you want to use for running this model using multi-GPU inference.
    dtype: auto  # https://docs.vllm.ai/en/latest/models/engine_args.html#cmdoption-dtype
    jud_res_cache_file: jud_results.csv  # cache judgment results file
    ver_res_cache_file: verify_results.csv  # the file for caching the judge results after verification. Each judge model will have a such file.
    verify_convs_cache_file: verify_convs.csv  # the file for caching the verification conversations. Each judge model will have a such file.


  # ==========================================
  # add more

# pre-process config
preprocess:
  in_dirs:  # input directory in stage1
    - ./data/edited_release/distant_supervision
    - ./data/edited_release/crowd
    - ./data/edited_release/ontonotes
  out_dir: ./data/preprocessed

# each stage has its own config, which can be fn_kwargs to be passed to the process function
stage1:  # stage1 config
  # specify number of workers corresponding to your CPU cores.
  # Because all the stage of processing requires the participation of GPU,
  # We recommend specifying num_workers as the number of your GPUs. Otherwise, there is a risk of OOM.
  num_workers: 8

  # strict or loose. In strict mode, we get spans based on intersection of spaCy and Stanza results.
  # In loose mode, we get spans based on union of spaCy and Stanza results.
  mode: loose  # strict or loose
  in_dir: ./data/preprocessed  # input directory for stage1, which are results from preprocess stage
  sent_min_len: 15  # minimum length of sentence. Sentences with length less than min_len will be discarded
  span_portion: 0.3  # The proportion of the longest span length to sentence length. To filter out long span.
  batch_num_per_device: 20  # Specify number of batches on each device to be processed in map function. Depends on your RAM memory
  batch_size_per_device: 1024  #  Specify batch size per device. Depends on your GPU memory.
  spacy_model:
    # https://spacy.io/api/top-level#spacy.load
    # for spacy.load function, we need to specify 'name'
    name: en_core_web_trf  # for English, we can use en_core_web_sm, en_core_web_md, en_core_web_lg, en_core_web_trf
    disable: # disable some components to speed up, https://spacy.io/usage/processing-pipelines#disabling
      - tagger
      - parser
      - ner
      - attribute_ruler
      - lemmatizer
  stanza_model:
    # https://stanfordnlp.github.io/stanza/pipeline.html
    lang: en  # for English
    processors: tokenize, ner, pos, constituency  # specify processors, comma-seperated
    # dir: /data1/gcchen/stanza_data  # default ~/stanza_resources

    # tokenizer init
    # https://stanfordnlp.github.io/stanza/tokenize.html#options
    tokenize_pretokenized: True
    tokenize_no_ssplit: True  # https://stanfordnlp.github.io/stanza/tokenize.html#tokenization-without-sentence-segmentation

stage2:  # stage2 config
  # specify number of workers corresponding to your CPU cores.
  # Because all the stage of processing requires the participation of GPU,
  # We recommend specifying num_workers as the number of your GPUs. Otherwise, there is a risk of OOM.
  num_workers: 2

  anno_model: mistral  # choose annotate model from your config below.

  # In strict mode, the dataset contains fewer nested entities.
  # In loose mode, the dataset contains many nested entities
  mode: strict  # strict or loose. Strict mode
  in_dir: ./data/stage1  # input directory for stage2, which are results from stage1
  batch_num_per_device: 2000  # Specify number of batches on each device to be processed in map function. Depends on your RAM memory
  batch_size_per_device: 32  #  Specify batch size per device. Depends on your GPU memory.

  #annotate model configs
  Mistral:
    checkpoint: ./ckpt/TheBloke/Mixtral-8x7B-Instruct-v0.1-AWQ  # https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-AWQ
    # https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-AWQ#prompt-template-mistral
    anno_usr_prompt_batch: >-
      You are a professional and helpful English linguist. 
      You are about to read a sentence with an entity mention marked with two special symbols (i.e, [ent] and [/ent]),
      where '[ent]' indicates the start of the entity mention and '[/ent]' indicates the end of the entity mention.
      The sentence comes after the delimiter "---" and follow by the candidate type words and their corresponding definition.
      You should complete an entity typing task by performing the following actions: 
      1) First, you should read the candidate type words and their corresponding definition. The given type words are followed 
      by the sentence and each type word is given in the format of '<id>: the id of the type word ,<word>: the candidate type word, <definition>: the definition of the type word'.
      2) Second, you should determine which types the entity mention belongs to within the candidate type words according 
      to the given definition.
      3) If there is at least one type word within the candidate type words that the entity mention belongs to, output the ids 
      of these type words. Otherwise, output -1. 
      4)  You should explain your analysis process and then output your answer. Your output format is a json string, like
        '{"analysis": "your analysis process", "answer": "your answer"}'
      ### Start examples
      1)
      "While Goosen was busy measuring out his plots , [ent] the Duke of Edinburgh [/ent] , Prince Alfred of Edinburgh , visited the Cape Colony ."
      candidate type words:
        <id>: 0, <word>: person, <definition>: The physical body of a being seen as distinct from the mind, character, etc.
        <id>: 1, <word>: object, <definition>: An instantiation of a class or structure.
        <id>: 2, <word>: object, <definition>: An element within a category upon which functions operate. Thus, a category consists of a set of element objects and the functions that operate on them.
        <id>: 3, <word>: place, <definition>: (physical) An area; somewhere within an area.
        <id>: 4, <word>: place, <definition>: A chess position; a square of the chessboard.
      output:
        {
          "analysis": "The entity mention 'the Duke of Edinburgh' matches the definition 'The physical body of a being seen as distinct from the mind, character, etc.', which belongs to the type word 'person' with id 0. Furthermore, the entity mention does not match the definitions of other candidate type words. So I should output the anwser 0",
          "answer": "0"
          }
      
      2)
      "The station is the rebuilt [ent] Dundee Tay Bridge [/ent] railway station , which had been built by the North British Railway .:
        <id>: 2781, <word>: station, <definition>: Any of the Stations of the Cross. 
        <id>: 2782, <word>: station, <definition>: A church in which the procession of the clergy halts on stated days to say stated prayers.
        <id>: 2783, <word>: station, <definition>: Any of a sequence of equally spaced points along a path.
        <id>: 2784, <word>: station, <definition>: (physical) An area; somewhere within an area.
        <id>: 2785, <word>: station, <definition>: A chess position; a square of the chessboard.
      output:
        2782
        
      
      
      ### End examples

      ---
      definition A: {first_definition} 
      definition B: {second_definition}
      ---

    anno_temperature: 0.1  # temperature for this model. We expect the judge model output deterministic results, so we set temperature to 0.1.
    anno_top_p: 0.6  # top_p for this model. The smaller the value, the more deterministic the model output is.
    anno_max_tokens: 3  # We only need to output 1 or 0, so we set max_tokens to 5.
    anno_bs: 24  # batch size for this model
    tensor_parallel_size: 2  # the number of GPUs you want to use for running this model using multi-GPU inference.
    dtype: auto  # https://docs.vllm.ai/en/latest/models/engine_args.html#cmdoption-dtype
    anno_res_cache_file: jud_results.csv  # cache judgment results file

  #  ==========================================
  Qwen:
    # https://huggingface.co/Qwen/Qwen1.5-72B-Chat-AWQ
    checkpoint: ./ckpt/Qwen/Qwen1.5-72B-Chat-AWQ  # your path to the model checkpoint
    # qwen has system prompt. We can input the examples in a form of chatting
    # https://huggingface.co/Qwen/Qwen1.5-72B-Chat-AWQ#quickstart
    anno_sys_prompt: >-
      You are a professional and helpful English linguist. 
      You are about to read two definitions (i.e., definition A and definition B).
      Please judge whether two definitions have similar meanings or explaining a same noun word by performing the following actions: 
      1) Think the first question carefully: Are the meanings of these two definitions similar? 
      2) Think the second question carefully: Are the two definitions explaining a same noun?  
      3) If you answer "Yes" to one of the above questions, only output 1; Otherwise, only output 0.
      4) Please note that expect for 0 or 1, all other text, punctuation, numbers, and whitespace characters are not allowed in the output.

    # the examples can be input in a form of chatting
    anno_examples: # examples input in a form of chatting
      - definition_A: Any great, strong, powerful emotion, especially romantic love or extreme hate.
        definition_B: A passionate individual.
        output: 0
      - definition_A: A cargo ship or freighter is a merchant ship that carries cargo, goods, and materials from one port to another. Thousands of cargo carriers ply the world's seas and oceans each year, handling the bulk of international trade.
        definition_B: Any vessel designed to carry cargo.
        output: 1
      - definition_A: A flammable liquid consisting of a mixture of refined petroleum hydrocarbons, mainly used as a motor fuel; petrol.
        definition_B: Petroleum, a fluid consisting of a mixture of refined petroleum hydrocarbons, primarily consisting of octane, commonly used as a motor fuel.
        output: 1
      - definition_A: A shop or store that sells groceries; a grocery store.
        definition_B: a shop that sells cakes.
        output: 0
    anno_usr_prompt: >-
      definition A: {first_definition} 
      definition B: {second_definition}

    anno_verify_prompt: >-
      Your answer is not allowed. Please think again and only output 0 or 1. 

    anno_temperature: 0.1  # temperature for this model. We expect the judge model output deterministic results, so we set temperature to 0.1.
    anno_top_p: 0.6  # top_p for this model. The smaller the value, the more deterministic the model output is.
    anno_max_tokens: 3  # We only need to output 1 or 0, so we set max_tokens to 5.
    anno_bs: 24  # batch size for this model
    tensor_parallel_size: 2  # the number of GPUs you want to use for running this model using multi-GPU inference.
    dtype: auto  # https://docs.vllm.ai/en/latest/models/engine_args.html#cmdoption-dtype
    anno_res_cache_file: jud_results.csv  # cache judgment results file

  Yi:
    checkpoint: ./ckpt/01-ai/Yi-34B-Chat-8bits  # https://huggingface.co/01-ai/Yi-34B-Chat-8bits
    # https://huggingface.co/01-ai/Yi-34B-Chat-8bits#perform-inference-with-yi-chat-model
    anno_usr_prompt: >-
      You are a professional and helpful English linguist. 
      You diligently complete an entity typing task as instructed. 
      You are about to read a sentence with a given entity mention marked with two special symbols (i.e, [ent] and [/ent]),
      where '[ent]' indicates the start of the entity mention and '[/ent]' indicates the end of the entity mention.
      Please judge the type of the given entity mention in that sentence by performing the following actions: 
      1) Read the candidate type words and their corresponding definition carefully.
      2) Think the second question carefully: Are the two definitions explaining a same noun?  
      3) If you answer "Yes" to one of the above questions, only output 1; Otherwise, only output 0.
      4) Please note that expect for 0 or 1, all other text, punctuation, numbers, and whitespace characters are not allowed in the output.

      ### Start examples
      definition A: Any great, strong, powerful emotion, especially romantic love or extreme hate.
      definition B: A passionate individual.
      0

      definition A: A cargo ship or freighter is a merchant ship that carries cargo, goods, and materials from one port to another. Thousands of cargo carriers ply the world's seas and oceans each year, handling the bulk of international trade.
      definition B: Any vessel designed to carry cargo.
      1

      definition A: A flammable liquid consisting of a mixture of refined petroleum hydrocarbons, mainly used as a motor fuel; petrol.
      definition B: Petroleum, a fluid consisting of a mixture of refined petroleum hydrocarbons, primarily consisting of octane, commonly used as a motor fuel.
      1

      definition A: A shop or store that sells groceries; a grocery store.
      definition B: a shop that sells cakes.
      0
      ### End examples

      ---
      definition A: {first_definition} 
      definition B: {second_definition}
      ---

    anno_temperature: 0.1  # temperature for this model. We expect the judge model output deterministic results, so we set temperature to 0.1.
    anno_top_p: 0.6  # top_p for this model. The smaller the value, the more deterministic the model output is.
    anno_max_tokens: 3  # We only need to output 1 or 0, so we set max_tokens to 5.
    anno_bs: 24  # batch size for this model
    tensor_parallel_size: 2  # the number of GPUs you want to use for running this model using multi-GPU inference.
    dtype: auto  # https://docs.vllm.ai/en/latest/models/engine_args.html#cmdoption-dtype
    anno_res_cache_file: jud_results.csv  # cache judgment results file