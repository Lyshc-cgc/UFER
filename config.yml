num_workers: 8  # specify number of workers corresponding to your CPU cores. Recommend be the number of your GPUs.

# We can split large files into small shards to avoid memory overflow and boost process speed.
# specify number of shards to be processed in map function. Depends on your memory.
# If you don't want to split, you can set num_shards to 1.
# the larger dataset you have, the larger num_shards you need.
# Note that it must be multiples of num_worker. e.g., 1*num_workers, 2*num_workers
num_shards: 8
# if you have multiple (e.g., 4) GPUs, you can specify some of them(e.g. 0, 1, 2) or all of them (e.g., all)
cuda_device: all  # specify visible GPU device. If you have multiple GPUs, you can specify some of them (e.g. 0, 1, 2) or all of them (e.g., all)
data_dir: ./data  # data directory

type_cfg:  # UFER type config
  type_vocab: ./data/ufer_types/type_vocab.txt  # type vocabulary file, which stores all the type words in UFER
  type_info_file: ./data/ufer_types/test_type_info.jsonl  # type information file, which stores information of all the types in UFER

  # dictionary api, which is used to get professional information for type words
  # we use a free dictionary api {https://github.com/meetDeveloper/freeDictionaryAPI}
  api_url: https://api.dictionaryapi.dev/api/v2/entries/  # API URL of the free Dictionary API
  lang_type: en  # language type of the dictionary you want, 'en' for English dictionary

  # wikipedia api config, which is used to get wikipedia information for type words
  # see https://wikipedia.readthedocs.io/en/latest/quickstart.html for more details
  sentences: 2  # number of sentences to be extracted from Wikipedia Encyclopedia
  auto_suggest: False  # whether to automatically search for alternative suggestions if the query word is not found. Default is False.

# pre-process config
preprocess:
  in_dirs:  # input directory in stage1
    - ./data/edited_release/distant_supervision
    - ./data/edited_release/crowd
    - ./data/edited_release/ontonotes
  out_dir: ./data/preprocessed

# each stage has its own config, which can be fn_kwargs to be passed to the process function
stage1:  # stage1 config
  # strict or loose. In strict mode, we get spans based on intersection of spaCy and Stanza results.
  # In loose mode, we get spans based on union of spaCy and Stanza results.
  mode: loose  # strict or loose
  in_dir: ./data/preprocessed  # input directory in stage1, which are results from preprocess stage
  sent_min_len: 15  # minimum length of sentence. Sentences with length less than min_len will be discarded
  span_portion: 0.3  # The proportion of the longest span length to sentence length. To filter out long span.
  batch_num_per_device: 10  # Specify number of batches on each device to be processed in map function. Depends on your memory
  batch_size_per_device: 1024  #  Specify batch size per device. Depends on your GPU memory.
  spacy_model:
    # https://spacy.io/api/top-level#spacy.load
    # for spacy.load function, we need to specify 'name'
    name: en_core_web_trf  # for English, we can use en_core_web_sm, en_core_web_md, en_core_web_lg, en_core_web_trf
    disable: # disable some components to speed up, https://spacy.io/usage/processing-pipelines#disabling
      - tagger
      - parser
      - ner
      - attribute_ruler
      - lemmatizer
  stanza_model:
    # https://stanfordnlp.github.io/stanza/pipeline.html
    lang: en  # for English
    processors: tokenize, ner, pos, constituency  # specify processors, comma-seperated
    # dir: /data1/gcchen/stanza_data  # default ~/stanza_resources

    # tokenizer init
    # https://stanfordnlp.github.io/stanza/tokenize.html#options
    tokenize_pretokenized: True
    tokenize_no_ssplit: True  # https://stanfordnlp.github.io/stanza/tokenize.html#tokenization-without-sentence-segmentation

